---
layout: post
title: "Alliterative Allure Aside"
categories: musings
author:
- Leo
---

<img src="/assets/2024-09-30-alliterative-allure-aside/header.jpeg" class="responsive">

I love the "cannot" stances.
Those naysayers who draw lines in the sand as if exploration bows to their whims.
Maybe it's easier for some to zero in on clipping the wings of possibility than to watch others use them to soar.
But blame has no place in this.
Computation is in the eye of the beholder.
And just as that eye relies on being struck by rays of light to function, computation serves as the pinnacle of our understanding.
There's nothing that strikes me as more fundamental than the concept of computation.
It's our modern ultra-accelerationist [categorical imperative](https://www.wikiwand.com/en/articles/Categorical_imperative).

I personally see an unbounded future, an anisotropic positive divergence spreading wildly.
LLMs represent a milestone in this present moment.
Mastering language is the skeleton key unlocking the true binding between all other modalities.
It's the Rosetta Stone of AI.
Vann McGee presents a compelling argument.
Verbal reasoning, when you strip it down, thrives on the rules we construct.
He [wrote](https://ocw.mit.edu/courses/24-242-logic-ii-spring-2004/20c09c32f5c237d1fb4207a83153dbb5_why_study_comptt.pdf) that,
> "Recursion theory is concerned with problems that can be solved by following a rule or a system of rules.
> Linguistic conventions, in particular, are rules for the use of a language, and so human language is the sort of rule-governed behavior to which recursion theory applies.
> Thus, if, as seems likely, an English-speaking child learns a system of rules that enable her to tell which strings of English words are English sentences, then the set of English sentences has to be a decidable set.
> This observation puts nontrivial constraints upon what the grammar of a natural language can look like.
> As Wittgenstein never tired of pointing out, when we learn the meaning of a word, we learn how to use the word.
> That is, we learn a rule that governs the word's use."

Rules within rules.
A labyrinth of our own making.
But who says we can't rewrite them, bend them until they snap?
Language is alive, pulsing with the potential we inject into it.
As [Max Bennett](https://www.goodreads.com/book/show/62050269-a-brief-history-of-intelligence) notes,
> "We are capable of puppeteering other minds because language is, it seems, built right on top of a direct window to our inner simulation."

[The Model is the Message](https://www.noemamag.com/the-model-is-the-message/).
Viewing these models as mere autocompletion is shortsighted.
It grounds you, sure, but shackles you to a reality devoid of wonder.
It's like knowing we can't fly and using that as an excuse to never touch the sky.
No dreams of aeronautics, no reaching beyond.
These models are a different breed of autocompletion.
They're conditioned on us, both when they learn and when they respond.
They capture the subtle nuances, the intricate ways we weave meaning into words.
The "mind modeling" aspect is more like a warp around ourselves.
Think of it through the lens of conditional probability.
It's not just predicting the next word; it's predicting us.
The statistics are trying to satisfy our unspoken thoughts, to mirror the shadows of our minds.
Teach the model that a question has a certain answer.
Then you can flip it by asking what question leads to that answer.
Predicting which concepts align with specific properties—that's the autocompletion these models engage in.
Look at everyone who interacted with Sydney, pouring bits of themselves into the digital abyss.

But AI is much broader than that.
Why is it so difficult to imagine yet so easy to dream?
I believe part of the answer is hidden in [Siri](https://www.goodreads.com/book/show/48484.Blindsight)'s monologue, rendered in full below because it's just that good:
> "I am the bridge between the bleeding edge and the dead center.
> I stand between the Wizard of Oz and the man behind the curtain.
>
> I am the curtain.
>
>  I am not an entirely new breed.
>  My roots reach back to the dawn of civilization but those precursors served a different function, a less honorable one.
>  They only greased the wheels of social stability; they would sugarcoat unpleasant truths, or inflate imaginary bogeymen for political expedience.
>  They were vital enough in their way. Not even the most heavily-armed police state can exert brute force on all of its citizens all of the time.
>  Meme management is so much subtler; the rose-tinted refraction of perceived reality, the contagious fear of threatening alternatives.
>  There have always been those tasked with the rotation of informational topologies, but throughout most of history they had little to do with increasing its clarity.
>
>  The new Millennium changed all that.
>  We've surpassed ourselves now, we're exploring terrain beyond the limits of merely human understanding.
>  Sometimes its contours, even in conventional space, are just too intricate for our brains to track; other times its very axes extend into dimensions
>  inconceivable to minds built to fuck and fight on some prehistoric grassland.
>  So many things constrain us, from so many directions.
>  The most altruistic and sustainable philosophies fail before the brute brain-stem imperative of self-interest.
>  Subtle and elegant equations predict the behavior of the quantum world, but none can explain it.
>  After four thousand years we can't even prove that reality exists beyond the mind of the first-person dreamer.
>  We have such need of intellects greater than our own.
>
>  But we're not very good at building them.
>  The forced matings of minds and electrons succeed and fail with equal spectacle.
>  Our hybrids become as brilliant as savants, and as autistic.
>  We graft people to prosthetics, make their overloaded motor strips juggle meat and machinery, and shake our heads when their fingers twitch and their tongues stutter.
>  Computers bootstrap their own offspring, grow so wise and incomprehensible that their communiqués assume the hallmarks of dementia: unfocused and irrelevant to the barely-intelligent creatures left behind.
>
>  And when your surpassing creations find the answers you asked for, you can't understand their analysis and you can't verify their answers.
>  You have to take their word on faith—
>
>  Or you use information theory to flatten it for you, to squash the tesseract into two dimensions and the Klein bottle into three, to simplify reality and pray to whatever Gods survived the millennium that your honorable twisting of the truth hasn't ruptured any of its load-bearing pylons.
>  You hire people like me; the crossbred progeny of profilers and proof assistants and information theorists.
>
>  In formal settings you'd call me Synthesist.
>  On the street you call me jargonaut or poppy.
>  If you're one of those savants whose hard-won truths are being bastardized and lobotomized for powerful know-nothings interested only in market share, you might call me a mole or a chaperone.
>
>  If you're Isaac Szpindel you'd call me commissar, and while the jibe would be a friendly one, it would also be more than that.
>
>  I've never convinced myself that we made the right choice.
>  I can cite the usual justifications in my sleep, talk endlessly about the rotational topology of information and the irrelevance of semantic comprehension.
>  But after all the words, I'm still not sure.
>  I don't know if anyone else is, either.
>  Maybe it's just some grand consensual con, marks and players all in league.
>  We won't admit that our creations are beyond us; they may speak in tongues, but our priests can read those signs.
>  Gods leave their algorithms carved into the mountainside but it's just lil ol' me bringing the tablets down to the masses, and I don't threaten anyone.
>
>  Maybe the Singularity happened years ago.
>  We just don't want to admit we were left behind."

Some say we'd become feature vectors in a sprawling lookup table.
Is this where we're headed?
Reduced to algorithms and data points, our essence distilled into machine-readable patterns?

Perhaps. However,
> "Reality overstepping the boundaries of comfortable vocabulary is the start, not the end, of the conversation."

